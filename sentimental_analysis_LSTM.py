# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Y1jXixuEFOBnQ9EpbbQcKoXaZlpmg6m
"""

!pip install pandas matplotlib tensorflow
import json
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D
from tensorflow.keras.layers import Embedding
import matplotlib.pyplot as plt
df = pd.read_csv("Tweets.csv")

review_df = df[['text','airline_sentiment']]
print(review_df.shape)
review_df.head(5)

df.columns

review_df = review_df[review_df['airline_sentiment'] != 'neutral']
print(review_df.shape)
review_df.head(5)

review_df["airline_sentiment"].value_counts()

sentiment_label = review_df.airline_sentiment.factorize()
sentiment_label

tweet = review_df.text.values

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(tweet)
vocab_size = len(tokenizer.word_index) + 1

encoded_docs = tokenizer.texts_to_sequences(tweet)

padded_sequence = pad_sequences(encoded_docs, maxlen=200)

embedding_vector_length = 32
model = Sequential()
model.add(Embedding(vocab_size, embedding_vector_length, input_length=200))
model.add(SpatialDropout1D(0.25))
model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=5, batch_size=32)

plt.plot(history.history['accuracy'], label='acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.show()
plt.savefig("Accuracy plot.jpg")

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.show()
plt.savefig("Loss plt.jpg")

def predict_sentiment(text):
    tw = tokenizer.texts_to_sequences([text])
    tw = pad_sequences(tw,maxlen=200)
    prediction = int(model.predict(tw).round().item())
    print("Predicted label: ", sentiment_label[1][prediction])
    print(model.predict(tw).round().item()) 
    print(model.predict(tw))
    return model.predict(tw).round().item()
test_sentence1 = "I enjoyed my journey on this flight."
predict_sentiment(test_sentence1)
test_sentence2 = "This is the worst flight experience of my life!"
predict_sentiment(test_sentence2)

# Opening JSON file
f1 = open('/content/drive/MyDrive/data.json')
  
# returns JSON object as 
# a dictionary
data = json.load(f1)
f1.close()
print(data[0]['content'])

# Example to find the average of the list
from statistics import mean
mappings = {}
for i in data:
  results_prediction = []
  print(i)
  for j in i['content']:
    results_prediction.append(predict_sentiment(j))
  print(results_prediction)
  mappings[i['title']] = mean(results_prediction)

def classify_sentiment(value2):
  if round(value2)>0.5:
    return "Negative"
  else:
    return "Positive"

print("                   Headlines                      --------    Polarity    -------  Sentiment")
for key in mappings:
  print(key +" ------- "+ str(mappings[key]) + " ------- "+ classify_sentiment(mappings[key]))

